# normalization
norm_obs: true
norm_reward: false

# for network parameters
activation: LeakyReLU
hidden_dim: 256
log_std_init: -1.0
log_std_grad: false

# for training
train_frequency: 10000
minibatch_size: 128
discount_factor: 0.99
lr: 3.0e-4
train_epochs: 10
critic_coeff: 0.5
gae_coeff: 0.97
ent_coeff: 0.01
max_grad_norm: 1.0

# for trust region
max_kl: 0.01
kl_tolerance: 2.0
adaptive_lr_ratio: 2.0
clip_ratio: 0.2

# logger
log_list:
  rollout: [score, step, ep_len]
  train: [actor_loss, critic_loss, kl, entropy, epochs, lr]